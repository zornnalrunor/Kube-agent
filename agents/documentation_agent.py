"""
Documentation Agent
Agent responsable de la gÃ©nÃ©ration automatique de la documentation
"""
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from core.agent_base import AgentInput, AgentOutput, BaseAgent


class DocumentationAgent(BaseAgent):
    """
    Agent de documentation
    
    ResponsabilitÃ©s:
    - GÃ©nÃ©rer la documentation technique
    - CrÃ©er les runbooks
    - Documenter l'architecture dÃ©ployÃ©e
    - GÃ©nÃ©rer des diagrammes
    - CrÃ©er les guides d'opÃ©ration
    """
    
    def execute(self, agent_input: AgentInput) -> AgentOutput:
        """
        GÃ©nÃ¨re la documentation complÃ¨te
        
        Args:
            agent_input: Input contenant tous les outputs prÃ©cÃ©dents
            
        Returns:
            AgentOutput: Documentation gÃ©nÃ©rÃ©e
        """
        logs = []
        errors = []
        
        try:
            self.log("Generating documentation")
            
            # RÃ©cupÃ©rer tous les outputs
            planner_output = agent_input.previous_outputs.get("planner", {})
            infra_output = agent_input.previous_outputs.get("infrastructure", {})
            monitoring_output = agent_input.previous_outputs.get("monitoring", {})
            validation_output = agent_input.previous_outputs.get("validation", {})
            
            config = planner_output.get("optimized_config", agent_input.context)
            platform = config.get("platform", "k3s")
            environment = config.get("environment", "development")
            
            # CrÃ©er le rÃ©pertoire de documentation
            docs_dir = self._prepare_docs_directory(agent_input.workflow_id)
            logs.append(f"Documentation directory: {docs_dir}")
            
            # GÃ©nÃ©rer README principal
            self.log("Generating README...")
            readme_path = self._generate_readme(
                docs_dir,
                agent_input.workflow_id,
                config,
                planner_output,
                infra_output,
                monitoring_output,
                validation_output
            )
            logs.append(f"README generated: {readme_path}")
            self.log_success("README.md generated")
            
            # GÃ©nÃ©rer le document d'architecture
            self.log("Generating architecture documentation...")
            arch_path = self._generate_architecture_doc(
                docs_dir,
                config,
                infra_output,
                monitoring_output
            )
            logs.append(f"Architecture doc: {arch_path}")
            self.log_success("ARCHITECTURE.md generated")
            
            # GÃ©nÃ©rer le runbook opÃ©rationnel
            self.log("Generating operational runbook...")
            runbook_path = self._generate_runbook(
                docs_dir,
                config,
                infra_output,
                monitoring_output
            )
            logs.append(f"Runbook: {runbook_path}")
            self.log_success("RUNBOOK.md generated")
            
            # GÃ©nÃ©rer le guide de troubleshooting
            self.log("Generating troubleshooting guide...")
            troubleshooting_path = self._generate_troubleshooting(
                docs_dir,
                platform,
                monitoring_output
            )
            logs.append(f"Troubleshooting: {troubleshooting_path}")
            self.log_success("TROUBLESHOOTING.md generated")
            
            # GÃ©nÃ©rer les configurations exportÃ©es
            self.log("Exporting configurations...")
            config_path = self._export_configurations(
                docs_dir,
                agent_input.workflow_id,
                config,
                infra_output
            )
            logs.append(f"Configurations exported: {config_path}")
            self.log_success("Configurations exported")
            
            # GÃ©nÃ©rer un diagramme d'architecture ASCII
            self.log("Generating architecture diagram...")
            diagram = self._generate_architecture_diagram(config, monitoring_output)
            diagram_path = docs_dir / "ARCHITECTURE_DIAGRAM.txt"
            diagram_path.write_text(diagram)
            logs.append(f"Diagram: {diagram_path}")
            self.log_success("Architecture diagram generated")
            
            # Liste de tous les fichiers gÃ©nÃ©rÃ©s
            generated_files = list(docs_dir.glob("*"))
            
            return AgentOutput(
                agent_name=self.agent_name,
                success=True,
                data={
                    "docs_directory": str(docs_dir),
                    "readme_path": str(readme_path),
                    "architecture_path": str(arch_path),
                    "runbook_path": str(runbook_path),
                    "troubleshooting_path": str(troubleshooting_path),
                    "config_path": str(config_path),
                    "generated_files": [str(f) for f in generated_files],
                    "summary": f"Documentation generated in {docs_dir.name}"
                },
                errors=errors,
                logs=logs,
            )
            
        except Exception as e:
            error_msg = f"Documentation generation failed: {str(e)}"
            errors.append(error_msg)
            self.log_error(error_msg)
            
            return AgentOutput(
                agent_name=self.agent_name,
                success=False,
                errors=errors,
                logs=logs,
            )
    
    def _prepare_docs_directory(self, workflow_id: str) -> Path:
        """PrÃ©pare le rÃ©pertoire de documentation"""
        docs_dir = self.config.output_dir / "docs" / workflow_id
        docs_dir.mkdir(parents=True, exist_ok=True)
        return docs_dir
    
    def _generate_readme(
        self,
        docs_dir: Path,
        workflow_id: str,
        config: Dict[str, Any],
        planner_output: Dict[str, Any],
        infra_output: Dict[str, Any],
        monitoring_output: Dict[str, Any],
        validation_output: Dict[str, Any]
    ) -> Path:
        """GÃ©nÃ¨re le README principal"""
        
        platform = config.get("platform", "k3s")
        environment = config.get("environment", "development")
        nodes = config.get("nodes", 1)
        
        grafana_url = monitoring_output.get("grafana_url", "N/A")
        prometheus_url = monitoring_output.get("prometheus_url", "N/A")
        health_score = validation_output.get("health_score", "N/A")
        
        content = f"""# Cluster Kubernetes - {workflow_id}

## ğŸ“‹ Informations GÃ©nÃ©rales

- **Workflow ID**: `{workflow_id}`
- **Plateforme**: {platform.upper()}
- **Environnement**: {environment}
- **Nombre de nÅ“uds**: {nodes}
- **Date de crÃ©ation**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **Score de santÃ©**: {health_score}/100

## ğŸ—ï¸ Architecture

Ce cluster a Ã©tÃ© automatiquement provisionnÃ© et configurÃ© via le systÃ¨me Terraform K8s Agent.

### Composants DÃ©ployÃ©s

#### Infrastructure
- **Plateforme**: {platform}
- **Nodes**: {nodes} nÅ“uds
- **Version Kubernetes**: {config.get('kubernetes_version', '1.28')}

#### Monitoring
- **Prometheus**: {prometheus_url}
- **Grafana**: {grafana_url}
  - Username: `admin`
  - Password: `{config.get('monitoring', {}).get('grafana_password', 'admin')}`

#### Addons
"""
        
        addons = config.get('addons', {})
        for addon, enabled in addons.items():
            if enabled:
                content += f"- âœ… {addon}\n"
        
        content += f"""
## ğŸš€ AccÃ¨s au Cluster

### Kubeconfig

```bash
export KUBECONFIG={infra_output.get('kubeconfig_path', 'N/A')}
kubectl get nodes
kubectl get pods --all-namespaces
```

### Monitoring

#### Grafana
- URL: {grafana_url}
- Dashboards prÃ©-configurÃ©s:
"""
        
        for dashboard in monitoring_output.get('dashboards', []):
            content += f"  - {dashboard}\n"
        
        content += f"""
#### Prometheus
- URL: {prometheus_url}
- Targets: {validation_output.get('monitoring_status', {}).get('targets', {}).get('up', 'N/A')} up

## ğŸ“Š Ã‰tat du Cluster

### NÅ“uds
```
{validation_output.get('nodes_ready', 'N/A')} nÅ“uds ready
```

### Pods
```
{validation_output.get('pods_running', 'N/A')} pods running
```

### CapacitÃ©
- **CPU**: {validation_output.get('capacity', {}).get('cpu', 'N/A')}
- **Memory**: {validation_output.get('capacity', {}).get('memory', 'N/A')}
- **Storage**: {validation_output.get('capacity', {}).get('storage', 'N/A')}

## ğŸ“š Documentation

- [Architecture dÃ©taillÃ©e](ARCHITECTURE.md)
- [Runbook opÃ©rationnel](RUNBOOK.md)
- [Guide de troubleshooting](TROUBLESHOOTING.md)
- [Configurations exportÃ©es](configs/)

## ğŸ”§ Commandes Utiles

### VÃ©rifier la santÃ© du cluster
```bash
kubectl get nodes
kubectl get pods --all-namespaces
kubectl top nodes
kubectl top pods --all-namespaces
```

### AccÃ©der aux logs
```bash
# Logs Prometheus
kubectl logs -n monitoring -l app=prometheus

# Logs Grafana
kubectl logs -n monitoring -l app=grafana
```

### Port-forwarding local
```bash
# Grafana
kubectl port-forward -n monitoring svc/grafana 3000:3000

# Prometheus
kubectl port-forward -n monitoring svc/prometheus 9090:9090
```

## ğŸ†˜ Support

En cas de problÃ¨me, consultez le [guide de troubleshooting](TROUBLESHOOTING.md) ou les logs des agents:
- Planner: Analyse et optimisation de la configuration
- Infrastructure: Provisioning Terraform
- Monitoring: DÃ©ploiement Prometheus/Grafana
- Validation: VÃ©rifications de santÃ©

## ğŸ—‘ï¸ Destruction

Pour dÃ©truire ce cluster:

```bash
cd {infra_output.get('workspace', 'N/A')}
terraform destroy -auto-approve
```

---
*Documentation gÃ©nÃ©rÃ©e automatiquement par Terraform K8s Agent*
"""
        
        readme_path = docs_dir / "README.md"
        readme_path.write_text(content)
        return readme_path
    
    def _generate_architecture_doc(
        self,
        docs_dir: Path,
        config: Dict[str, Any],
        infra_output: Dict[str, Any],
        monitoring_output: Dict[str, Any]
    ) -> Path:
        """GÃ©nÃ¨re la documentation d'architecture"""
        
        platform = config.get("platform", "k3s")
        
        content = f"""# Architecture du Cluster

## Vue d'Ensemble

Ce document dÃ©crit l'architecture du cluster Kubernetes dÃ©ployÃ© sur **{platform}**.

## Infrastructure

### Plateforme: {platform.upper()}

#### Configuration RÃ©seau
- **Pod CIDR**: {config.get('networking', {}).get('pod_cidr', '10.244.0.0/16')}
- **Service CIDR**: {config.get('networking', {}).get('service_cidr', '10.96.0.0/16')}

#### NÅ“uds
- **Nombre**: {config.get('nodes', 1)}
- **Type**: {config.get('resources', {}).get('instance_type', 'N/A')}
- **CPU**: {config.get('resources', {}).get('cpu', 'N/A')}
- **Memory**: {config.get('resources', {}).get('memory', 'N/A')}

## Stack Monitoring

### Prometheus
- **Namespace**: monitoring
- **Retention**: {config.get('monitoring', {}).get('retention', '15d')}
- **Scrape Interval**: 15s

#### Targets
- kubernetes-nodes
- kubernetes-pods
- kubernetes-services

### Grafana
- **Namespace**: monitoring
- **Datasource**: Prometheus (par dÃ©faut)
- **Dashboards**: {len(monitoring_output.get('dashboards', []))} prÃ©-configurÃ©s

## SÃ©curitÃ©

### RBAC
- **ActivÃ©**: {config.get('security', {}).get('rbac_enabled', True)}

### Network Policies
- **ActivÃ©es**: {config.get('security', {}).get('network_policies', False)}

### Pod Security
- **Policy activÃ©e**: {config.get('security', {}).get('pod_security_policy', False)}

## Addons

### InstallÃ©s
"""
        
        for addon, enabled in config.get('addons', {}).items():
            status = "âœ…" if enabled else "âŒ"
            content += f"- {status} **{addon}**\n"
        
        content += """
## Flux de DonnÃ©es

1. **Metrics Collection**: Les node-exporters et kube-state-metrics collectent les mÃ©triques
2. **Prometheus**: Scrape et stocke les mÃ©triques
3. **Grafana**: Visualise les mÃ©triques depuis Prometheus
4. **Alertmanager**: (si configurÃ©) GÃ¨re les alertes

## Haute DisponibilitÃ©

"""
        
        if config.get('environment') == 'production':
            content += """
- Multi-nodes pour la redondance
- Prometheus avec retention longue
- Sauvegardes automatiques configurÃ©es
"""
        else:
            content += """
- Configuration simple-node (non-production)
- Retention courte pour Ã©conomiser les ressources
"""
        
        content += """
## SchÃ©ma d'Architecture

Voir [ARCHITECTURE_DIAGRAM.txt](ARCHITECTURE_DIAGRAM.txt)

---
*GÃ©nÃ©rÃ© automatiquement*
"""
        
        arch_path = docs_dir / "ARCHITECTURE.md"
        arch_path.write_text(content)
        return arch_path
    
    def _generate_runbook(
        self,
        docs_dir: Path,
        config: Dict[str, Any],
        infra_output: Dict[str, Any],
        monitoring_output: Dict[str, Any]
    ) -> Path:
        """GÃ©nÃ¨re le runbook opÃ©rationnel"""
        
        content = f"""# Runbook OpÃ©rationnel

## ğŸ” Monitoring Quotidien

### VÃ©rifications JournaliÃ¨res

```bash
# VÃ©rifier les nÅ“uds
kubectl get nodes

# VÃ©rifier les pods critiques
kubectl get pods -n kube-system
kubectl get pods -n monitoring

# VÃ©rifier les events
kubectl get events --sort-by='.lastTimestamp'
```

### MÃ©triques Ã  Surveiller

#### Dans Grafana ({monitoring_output.get('grafana_url', 'N/A')})
- **CPU Utilization**: doit rester < 80%
- **Memory Utilization**: doit rester < 85%
- **Disk Usage**: doit rester < 80%
- **Pod Restarts**: max 3 restarts / 1h

#### Dans Prometheus ({monitoring_output.get('prometheus_url', 'N/A')})
- VÃ©rifier que tous les targets sont UP
- VÃ©rifier qu'il n'y a pas de gaps dans les mÃ©triques

## ğŸš¨ ProcÃ©dures d'Urgence

### NÅ“ud Down

```bash
# 1. Identifier le nÅ“ud
kubectl get nodes

# 2. VÃ©rifier les logs
kubectl describe node <node-name>

# 3. Drainer le nÅ“ud si nÃ©cessaire
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# 4. RedÃ©marrer ou remplacer le nÅ“ud
```

### Pod CrashLooping

```bash
# 1. Identifier le pod
kubectl get pods --all-namespaces | grep -v Running

# 2. VÃ©rifier les logs
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace> --previous  # Logs du container prÃ©cÃ©dent

# 3. DÃ©crire le pod
kubectl describe pod <pod-name> -n <namespace>

# 4. RedÃ©marrer si nÃ©cessaire
kubectl delete pod <pod-name> -n <namespace>
```

### Prometheus Down

```bash
# 1. VÃ©rifier le statut
kubectl get pods -n monitoring -l app=prometheus

# 2. VÃ©rifier les logs
kubectl logs -n monitoring -l app=prometheus

# 3. RedÃ©marrer
kubectl rollout restart deployment/prometheus -n monitoring
```

### Grafana Inaccessible

```bash
# 1. VÃ©rifier le service
kubectl get svc -n monitoring grafana

# 2. VÃ©rifier le pod
kubectl get pods -n monitoring -l app=grafana

# 3. Port-forward manuel
kubectl port-forward -n monitoring svc/grafana 3000:3000
```

## ğŸ”„ OpÃ©rations de Maintenance

### Mise Ã  Jour des Composants

#### Prometheus
```bash
# Edit la configuration
kubectl edit configmap prometheus-config -n monitoring

# Reload Prometheus
kubectl exec -n monitoring prometheus-0 -- kill -HUP 1
```

#### Grafana
```bash
# Mettre Ã  jour les dashboards
kubectl apply -f dashboards/

# RedÃ©marrer Grafana
kubectl rollout restart deployment/grafana -n monitoring
```

### Backups

#### Prometheus Data
```bash
# Snapshot des donnÃ©es
kubectl exec -n monitoring prometheus-0 -- curl -XPOST http://localhost:9090/api/v1/admin/tsdb/snapshot
```

#### Grafana Dashboards
```bash
# Export des dashboards via API
# (voir scripts de backup)
```

## ğŸ“ˆ Scaling

### Scale Up des NÅ“uds

```bash
# Modifier le count dans Terraform
cd {infra_output.get('workspace', 'N/A')}
# Ã‰diter terraform.tfvars: nodes = X
terraform apply
```

### Scale des Pods

```bash
# Scale horizontal
kubectl scale deployment <deployment-name> --replicas=X -n <namespace>

# Ou utiliser HPA
kubectl autoscale deployment <deployment-name> --min=2 --max=10 --cpu-percent=80 -n <namespace>
```

## ğŸ“Š Rapports

### GÃ©nÃ©rer un Rapport de SantÃ©

```bash
# Nodes
kubectl get nodes -o wide

# Pods
kubectl get pods --all-namespaces -o wide

# Ressources
kubectl top nodes
kubectl top pods --all-namespaces

# Events rÃ©cents
kubectl get events --sort-by='.lastTimestamp' | head -20
```

---
*Maintenir ce runbook Ã  jour aprÃ¨s chaque changement*
"""
        
        runbook_path = docs_dir / "RUNBOOK.md"
        runbook_path.write_text(content)
        return runbook_path
    
    def _generate_troubleshooting(
        self,
        docs_dir: Path,
        platform: str,
        monitoring_output: Dict[str, Any]
    ) -> Path:
        """GÃ©nÃ¨re le guide de troubleshooting"""
        
        content = f"""# Guide de Troubleshooting

## âŒ ProblÃ¨mes Courants

### 1. NÅ“ud Not Ready

**SymptÃ´mes**:
```bash
kubectl get nodes
NAME     STATUS     ROLES    AGE   VERSION
node-1   NotReady   <none>   1d    v1.28.0
```

**Diagnostic**:
```bash
# VÃ©rifier les dÃ©tails
kubectl describe node node-1

# VÃ©rifier kubelet
systemctl status kubelet

# VÃ©rifier les logs
journalctl -u kubelet -f
```

**Solutions**:
- RedÃ©marrer kubelet: `systemctl restart kubelet`
- VÃ©rifier la connectivity rÃ©seau
- VÃ©rifier les ressources disponibles (disk, memory)

### 2. Pod Pending

**SymptÃ´mes**:
```bash
kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
app-1   0/1     Pending   0          5m
```

**Diagnostic**:
```bash
kubectl describe pod app-1
# Regarder les Events
```

**Causes communes**:
- Ressources insuffisantes (CPU/Memory)
- Node selector ne matche aucun nÅ“ud
- PV non disponible
- Taints sur les nÅ“uds

**Solutions**:
- Ajouter des nÅ“uds
- Ajuster les requests/limits
- VÃ©rifier les labels et selectors

### 3. ImagePullBackOff

**SymptÃ´mes**:
```bash
NAME    READY   STATUS             RESTARTS   AGE
app-1   0/1     ImagePullBackOff   0          2m
```

**Diagnostic**:
```bash
kubectl describe pod app-1
# VÃ©rifier "Failed to pull image"
```

**Solutions**:
- VÃ©rifier que l'image existe
- VÃ©rifier les credentials (imagePullSecrets)
- VÃ©rifier la connectivity au registry

### 4. CrashLoopBackOff

**SymptÃ´mes**:
```bash
NAME    READY   STATUS              RESTARTS   AGE
app-1   0/1     CrashLoopBackOff    5          5m
```

**Diagnostic**:
```bash
# Logs actuels
kubectl logs app-1

# Logs du container prÃ©cÃ©dent
kubectl logs app-1 --previous

# Describe
kubectl describe pod app-1
```

**Solutions communes**:
- Corriger l'erreur applicative  
- Ajuster les probes (liveness/readiness)
- VÃ©rifier les variables d'environnement
- VÃ©rifier les volumes montÃ©s

### 5. Prometheus Ne Scrape Pas

**SymptÃ´mes**:
- Targets "Down" dans Prometheus
- MÃ©triques manquantes dans Grafana

**Diagnostic**:
```bash
# VÃ©rifier les targets
# Aller sur {monitoring_output.get('prometheus_url', 'N/A')}/targets

# VÃ©rifier les ServiceMonitors
kubectl get servicemonitors -n monitoring

# Logs Prometheus
kubectl logs -n monitoring -l app=prometheus
```

**Solutions**:
- VÃ©rifier les labels des services/pods
- VÃ©rifier les ServiceMonitors
- VÃ©rifier les NetworkPolicies
- RedÃ©marrer Prometheus

### 6. Grafana Dashboard Vide

**SymptÃ´mes**:
- Dashboards sans donnÃ©es
- "No data" dans les panels

**Diagnostic**:
```bash
# VÃ©rifier le datasource dans Grafana
# Settings > Data Sources

# Tester dans Prometheus directement
# Query: up{{}}
```

**Solutions**:
- VÃ©rifier que Prometheus scrape les donnÃ©es
- VÃ©rifier les queries PromQL
- VÃ©rifier la time range
- Refresh le datasource

## ğŸ” Commandes de Diagnostic

### Informations Cluster
```bash
kubectl cluster-info
kubectl version
kubectl get componentstatuses
```

### Ã‰tat des Ressources
```bash
kubectl get all --all-namespaces
kubectl get events --all-namespaces --sort-by='.lastTimestamp'
kubectl top nodes
kubectl top pods --all-namespaces
```

### Logs
```bash
# Logs d'un pod
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace> -f  # Follow
kubectl logs <pod-name> -n <namespace> --previous  # Container prÃ©cÃ©dent

# Logs d'un container spÃ©cifique
kubectl logs <pod-name> -c <container-name> -n <namespace>
```

### Exec dans un Pod
```bash
kubectl exec -it <pod-name> -n <namespace> -- /bin/sh
kubectl exec -it <pod-name> -n <namespace> -- /bin/bash
```

### Debug
```bash
# CrÃ©er un pod de debug
kubectl run debug --image=busybox --rm -it -- /bin/sh

# Debug rÃ©seau
kubectl run debug-net --image=nicolaka/netshoot --rm -it -- /bin/sh
```

## ğŸ“± Contacts & Escalade

### Niveau 1 - Self-Service
- Consulter ce guide
- Consulter le [Runbook](RUNBOOK.md)
- VÃ©rifier les logs et mÃ©triques

### Niveau 2 - Support
- Contacter l'Ã©quipe plateforme
- Fournir: workflow ID, logs, captures Grafana

### Niveau 3 - Escalade
- Incident critique affectant la production
- Perte de donnÃ©es
- Cluster inaccessible

## ğŸ”— Ressources Utiles

- Kubernetes Documentation: https://kubernetes.io/docs/
- Prometheus Documentation: https://prometheus.io/docs/
- Grafana Documentation: https://grafana.com/docs/

---
*Guide mis Ã  jour rÃ©guliÃ¨rement*
"""
        
        troubleshooting_path = docs_dir / "TROUBLESHOOTING.md"
        troubleshooting_path.write_text(content)
        return troubleshooting_path
    
    def _export_configurations(
        self,
        docs_dir: Path,
        workflow_id: str,
        config: Dict[str, Any],
        infra_output: Dict[str, Any]
    ) -> Path:
        """Exporte les configurations"""
        
        configs_dir = docs_dir / "configs"
        configs_dir.mkdir(exist_ok=True)
        
        # Config complÃ¨te en JSON
        config_json = configs_dir / "cluster-config.json"
        config_json.write_text(json.dumps(config, indent=2))
        
        # Info Terraform
        terraform_info = {
            "workspace": infra_output.get("workspace"),
            "outputs": infra_output.get("outputs", {}),
        }
        terraform_json = configs_dir / "terraform-info.json"
        terraform_json.write_text(json.dumps(terraform_info, indent=2))
        
        # Export metadata
        metadata = {
            "workflow_id": workflow_id,
            "created_at": datetime.now().isoformat(),
            "platform": config.get("platform"),
            "environment": config.get("environment"),
        }
        metadata_json = configs_dir / "metadata.json"
        metadata_json.write_text(json.dumps(metadata, indent=2))
        
        return configs_dir
    
    def _generate_architecture_diagram(
        self,
        config: Dict[str, Any],
        monitoring_output: Dict[str, Any]
    ) -> str:
        """GÃ©nÃ¨re un diagramme ASCII de l'architecture"""
        
        platform = config.get("platform", "k3s").upper()
        nodes = config.get("nodes", 1)
        
        diagram = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ARCHITECTURE - {platform} CLUSTER                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CONTROL PLANE                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  API Server  â”‚  â”‚  Scheduler   â”‚  â”‚   etcd       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WORKER NODE  â”‚  â”‚   WORKER NODE  â”‚  â”‚  WORKER NODE   â”‚
â”‚    (node-1)    â”‚  â”‚    (node-2)    â”‚  â”‚   (node-3)     â”‚
â”‚                â”‚  â”‚                â”‚  â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Kubelet â”‚  â”‚  â”‚  â”‚  Kubelet â”‚  â”‚  â”‚  â”‚  Kubelet â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Applicationâ”‚  â”‚  â”‚  â”‚Applicationâ”‚  â”‚  â”‚  â”‚Applicationâ”‚  â”‚
â”‚  â”‚   Pods   â”‚  â”‚  â”‚  â”‚   Pods   â”‚  â”‚  â”‚  â”‚   Pods   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONITORING STACK                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚     PROMETHEUS       â”‚â—„â”€â”€â”€â”‚       GRAFANA           â”‚    â”‚
â”‚  â”‚  (Metrics Storage)   â”‚    â”‚   (Visualization)       â”‚    â”‚
â”‚  â”‚                      â”‚    â”‚                         â”‚    â”‚
â”‚  â”‚  â€¢ Scrape Interval   â”‚    â”‚  â€¢ Dashboards           â”‚    â”‚
â”‚  â”‚  â€¢ Alert Rules       â”‚    â”‚  â€¢ User Auth            â”‚    â”‚
â”‚  â”‚  â€¢ Retention: {config.get('monitoring', {}).get('retention', '15d'):6} â”‚    â”‚  â€¢ Datasources          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â–²                            â”‚                      â”‚
â”‚           â”‚ scrapes                    â”‚ queries              â”‚
â”‚           â”‚                            â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  ServiceMonitor â”‚         â”‚     Users        â”‚           â”‚
â”‚  â”‚   kube-state    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚  â”‚   node-exporter â”‚                                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      NETWORKING                               â”‚
â”‚  Pod CIDR:     {config.get('networking', {}).get('pod_cidr', '10.244.0.0/16'):20}                       â”‚
â”‚  Service CIDR: {config.get('networking', {}).get('service_cidr', '10.96.0.0/16'):20}                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
  â–² = Data flow up
  â–¼ = Data flow down
  â—„â”€ = Connection
  â””â”€ = Hierarchy
"""
        
        return diagram
